### 1. A brief on the approach, which you used to solve the problem
My model is a quite straightforward variant of an RFM model. 
It combines customer repeat purchase behavior and items popular close to the data cutoff.

An important part that may not be obvious to everybody, yet is crucial for machine learning, was that I created a validation set that looked like the way the test set was described. 
We were told two months of repeat users, so I took the final two months of the training set as validation and removed all data for users who were not in both portions (validation and revised training). 
This led to a fairly small training set, but it seemed appropriate to mimic the test setup as closely as possible. My local MAPK was significantly higher, but directionally similar in that improvements locally led to improvements on the leaderboard in a ratio similar enough that I was comfortable with it. Indeed, the private leaderboard scores were a little higher, though still not as high as my internal validation scores.

### 2. Which data-preprocessing / feature engineering ideas really worked? How did you discover them?
The "R" of RFM proved to be an important part of the problem, so working with the date helped. I ordered by time with both the User portion and the Product portion of the model.

I created a lot of typical counts that I usually use, counts per user, per product, per user-product, and per season. But I wound up landing an a fairly strong model straightforward early on and never wound up using any of those features in a trained model. In such a recommender setup, having calibrated predictions is not important if one will display the Top K items to a returning user. So the value of a machine learning approach over a simple ordered approach is not as high other problem spaces.

### 3. How does your final model look like? How did you reach it?
My model is just a heuristic model, without machine learning. It is a two-part method of creating the ten product recommendations by most recent user's historical purchases, then if 10 items was not reached, filling the remaining slots with the most frequently purchased items in the last 10 days.
The path to reaching the model I started with was to start with something basic. My first submission was first on the public leaderboard fairly early on and it seemed appropriately simple at the time once I confirmed that many users buy things they had previously purchased; this model was submitted it without scoring anything internally. As it slide down the leaderboard, I noticed a mistake I had made in creating the submission, and when correcting that, I started measuring the MAP@10 scores and found that my FRm (frequency, recency) model could be improved by rearranging the ordering to RFm. The leaderboard showed similar improvement so I tried a couple other ways to incorporate recency--the best being limiting the most frequenty items across all users to just the most frequent items within the last month, which was later refined to just the last 10 days.

I spent some time looking at the patterns across the Season variable within the product table. Season 9 exhibited a strongly increasing trend; Seasons 5 & 6 peaked in June and July, but were starting to show signs of rebounding; Season 11 also peaked in July, but had continued to decrease. I thought about ways to further integrate the Season within my model, particularly by identifying if a User was purchasing in-season products, and adjusting their recommendations to more heavily focus on a particular season. But with the test set spanning two months and two months for which we had not seen the dominant season, I decided to leave this alone, especially in the interest of time and a robust model. Specifically I was a bit worried if one of the seasons might be Festival Season and it could become quite different in the test set.

![image](https://user-images.githubusercontent.com/2976822/51584881-d5ab1000-1e9c-11e9-8603-a2ec47bacd94.png)
Season over time (MMD, where D is a day of month bucket 0/1/2: 01-09/10-19/20-31)

### 4. What are the key takeaways from the challenge, if any?
I like problems where straightforward solutions can present a strong model. I consider this a simple approach, yet one that fits the data and time allotted. Though I did not use the images, other than to glance at a few of the most popular purchases, I like that this dimension was available to participants (and we can see one successful approach to similarity performed well). No doubt a more comprehensive solution, ensembling multiple approaches to the problem could yield an even stronger solution.

### 5. According to you, what are the 5 things a participant must focus on while solving such problems?
1. Understanding the data. Seeing that we only had a portion of one year was important.
2. Understanding the problem. Knowing that we were sure to see repeat users is an important key.
3. Create some simple baselines. I often use some flavor of GBM, but I feel the most in control of a problem when I have created some simple baseline models to see how well those score. My entire model can be considered one such model this time. But it's useful to see how well heuristics like guessing only the top product, or just top 10, or just repeat purchases would do in understanding the dynamics of the problem, prior to machine learning. I try to always do this when modeling.
4. Spend time creating strong local validation. In competitions and real world problems, it is crucially important to mimic your "production" scenario as closely as you can when validating models. In this case our "production" environment is the test set with guidelines provided to us. For some problems randomly sampling is the right choice. Very often, it is not and models or features that take advantage of improper dynamics may behave differently when you use them in production.
5. How time affects the data. Similar to the prior point, many real world problems exhibit different characteristics over time, and it is almost worth exploring how the dynamics of your data change over time. Seeing how popular items rise and fall is helpful in a recommender setup. Noticing that often when a user repeats a purchase, it is very quickly after the original purhase. Admittedly, that is a caution against weighting repeat buying habits too strongly, which I didn't use, but it was helpful to observe this, since my first MAPK scores were very high by just using repeat purchases, but I had not properly set up my validation environment to create a two-month break at that point. When I noticed that portion of the test setup, it made sense to me that my MAPK score would drop so much.
